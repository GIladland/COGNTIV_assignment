{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cognitv"
      ],
      "metadata": {
        "id": "EXH38BnSKs-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Kj0-u9IjKrCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "please put image files path: /content/gdrive/MyDrive/cognitv/48\n",
        "and \"/content/gdrive/MyDrive/cognitv/255"
      ],
      "metadata": {
        "id": "g_u30C7MEFBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files_path = \"/content/gdrive/MyDrive/cognitv\""
      ],
      "metadata": {
        "id": "EymA5MxvD9fA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "id": "-Okbrz70gZYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0szbdnD80BI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "from skimage import io\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Resize, Compose, ToTensor, Normalize\n",
        "import numpy as np\n",
        "import skimage\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import time\n",
        "from torchmetrics import PeakSignalNoiseRatio\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "qtXEd8QqgEAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cognitv solution\n",
        "\n",
        "Overview:\n",
        "\n",
        "Neural networks can be used for signal representation, in many domains. These approaches have several advantages over other traditional lossy methods. Examples of this concept are presented by Bricman et al. for representing single images, and similarly by Sitzmann et al. using sinusoidal activation functions for this task.\n",
        "\n",
        "\n",
        "* [The \"Naive\" Solution](#naive_solution)\n",
        "* [Probing the \"Naive\" implicit function](#probing_solution)\n",
        "* [Improving the solution - A fully connected Sinus Activation Module with entity embedding for different images](#improving_solution)\n",
        "* [Interpolation and Similarity measures for representations](#interpolation )\n"
      ],
      "metadata": {
        "id": "Ts1PP7LK7r9-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdWV26r0O85T"
      },
      "source": [
        "<a name=\"naive_solution\"></a>\n",
        "# Naive Solution"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mgrid(sidelen, dim=2):\n",
        "    '''Generates a flattened grid of (x,y,...) coordinates in a range of -1 to 1.\n",
        "    sidelen: int\n",
        "    dim: int'''\n",
        "    tensors = tuple(dim * [torch.linspace(-1, 1, steps=sidelen)])\n",
        "    mgrid = torch.stack(torch.meshgrid(*tensors), dim=-1)\n",
        "    mgrid = mgrid.reshape(-1, dim)\n",
        "    return mgrid"
      ],
      "metadata": {
        "id": "WSUEvauJ8Sch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SineLayer(nn.Module):\n",
        "    # See paper sec. 3.2, final paragraph, and supplement Sec. 1.5 for discussion of omega_0.\n",
        "    \n",
        "    # If is_first=True, omega_0 is a frequency factor which simply multiplies the activations before the \n",
        "    # nonlinearity. Different signals may require different omega_0 in the first layer - this is a \n",
        "    # hyperparameter.\n",
        "    \n",
        "    # If is_first=False, then the weights will be divided by omega_0 so as to keep the magnitude of \n",
        "    # activations constant, but boost gradients to the weight matrix (see supplement Sec. 1.5)\n",
        "    \n",
        "    def __init__(self, in_features, out_features, bias=True,\n",
        "                 is_first=False, omega_0=30):\n",
        "        super().__init__()\n",
        "        self.omega_0 = omega_0\n",
        "        self.is_first = is_first\n",
        "        \n",
        "        self.in_features = in_features\n",
        "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
        "        \n",
        "        self.init_weights()\n",
        "    \n",
        "    def init_weights(self):\n",
        "        with torch.no_grad():\n",
        "            if self.is_first:\n",
        "                self.linear.weight.uniform_(-1 / self.in_features, \n",
        "                                             1 / self.in_features)      \n",
        "            else:\n",
        "                self.linear.weight.uniform_(-np.sqrt(6 / self.in_features) / self.omega_0, \n",
        "                                             np.sqrt(6 / self.in_features) / self.omega_0)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        return torch.sin(self.omega_0 * self.linear(input))\n",
        "    \n",
        "    def forward_with_intermediate(self, input): \n",
        "        # For visualization of activation distributions\n",
        "        intermediate = self.omega_0 * self.linear(input)\n",
        "        return torch.sin(intermediate), intermediate\n",
        "    \n",
        "    \n",
        "class Siren(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features, hidden_layers, out_features, outermost_linear=False, \n",
        "                 first_omega_0=30, hidden_omega_0=30.):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.net = []\n",
        "        self.net.append(SineLayer(in_features, hidden_features, \n",
        "                                  is_first=True, omega_0=first_omega_0))\n",
        "\n",
        "        for i in range(hidden_layers):\n",
        "            self.net.append(SineLayer(hidden_features, hidden_features, \n",
        "                                      is_first=False, omega_0=hidden_omega_0))\n",
        "\n",
        "        if outermost_linear:\n",
        "            final_linear = nn.Linear(hidden_features, out_features)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                final_linear.weight.uniform_(-np.sqrt(6 / hidden_features) / hidden_omega_0, \n",
        "                                              np.sqrt(6 / hidden_features) / hidden_omega_0)\n",
        "                \n",
        "            self.net.append(final_linear)\n",
        "        else:\n",
        "            self.net.append(SineLayer(hidden_features, out_features, \n",
        "                                      is_first=False, omega_0=hidden_omega_0))\n",
        "        \n",
        "        self.net = nn.Sequential(*self.net)\n",
        "    \n",
        "    def forward(self, coords):\n",
        "        coords = coords.clone().detach().requires_grad_(True) # allows to take derivative w.r.t. input\n",
        "        output = self.net(coords)\n",
        "        return output, coords        \n",
        "\n",
        "    def forward_with_activations(self, coords, retain_grad=False):\n",
        "        '''Returns not only model output, but also intermediate activations.\n",
        "        Only used for visualizing activations later!'''\n",
        "        activations = OrderedDict()\n",
        "\n",
        "        activation_count = 0\n",
        "        x = coords.clone().detach().requires_grad_(True)\n",
        "        activations['input'] = x\n",
        "        for i, layer in enumerate(self.net):\n",
        "            if isinstance(layer, SineLayer):\n",
        "                x, intermed = layer.forward_with_intermediate(x)\n",
        "                \n",
        "                if retain_grad:\n",
        "                    x.retain_grad()\n",
        "                    intermed.retain_grad()\n",
        "                    \n",
        "                activations['_'.join((str(layer.__class__), \"%d\" % activation_count))] = intermed\n",
        "                activation_count += 1\n",
        "            else: \n",
        "                x = layer(x)\n",
        "                \n",
        "                if retain_grad:\n",
        "                    x.retain_grad()\n",
        "                    \n",
        "            activations['_'.join((str(layer.__class__), \"%d\" % activation_count))] = x\n",
        "            activation_count += 1\n",
        "\n",
        "        return activations"
      ],
      "metadata": {
        "id": "lDAuJ3yU0RF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def laplace(y, x):\n",
        "    grad = gradient(y, x)\n",
        "    return divergence(grad, x)\n",
        "\n",
        "\n",
        "def divergence(y, x):\n",
        "    div = 0.\n",
        "    for i in range(y.shape[-1]):\n",
        "        div += torch.autograd.grad(y[..., i], x, torch.ones_like(y[..., i]), create_graph=True)[0][..., i:i+1]\n",
        "    return div\n",
        "\n",
        "\n",
        "def gradient(y, x, grad_outputs=None):\n",
        "    if grad_outputs is None:\n",
        "        grad_outputs = torch.ones_like(y)\n",
        "    grad = torch.autograd.grad(y, [x], grad_outputs=grad_outputs, create_graph=True)[0]\n",
        "    return grad"
      ],
      "metadata": {
        "id": "jWFOo8nr0T5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-t83bO1EPFP5"
      },
      "source": [
        "## Combining all small 48 images to one big image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxnFz6ZeO-Pd"
      },
      "outputs": [],
      "source": [
        "def get_concat_h(im1, im2):\n",
        "    dst = Image.new('RGB', (im1.width + im2.width, im1.height))\n",
        "    dst.paste(im1, (0, 0))\n",
        "    dst.paste(im2, (im1.width, 0))\n",
        "    return dst\n",
        "\n",
        "def get_concat_v(im1, im2):\n",
        "    dst = Image.new('RGB', (im1.width, im1.height + im2.height))\n",
        "    dst.paste(im1, (0, 0))\n",
        "    dst.paste(im2, (0, im1.height))\n",
        "    return dst\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUJ4GdQDYO-a"
      },
      "outputs": [],
      "source": [
        "#big_img = np.array([0,0,4])\n",
        "path =  files_path + \"/48/*.*\"\n",
        "vertical_images = []\n",
        "for idx,file in enumerate(glob.glob(path)):\n",
        "   new_image = Image.fromarray(io.imread(file))\n",
        "   if idx == 0:\n",
        "      big_img = new_image\n",
        "      continue\n",
        "   elif idx> 0 and idx %10 == 0:\n",
        "     vertical_images.append(big_img)\n",
        "     #big_img = np.array([0,0,4])\n",
        "     big_img = new_image\n",
        "     continue\n",
        "   big_img = get_concat_h(big_img, new_image)\n",
        "vertical_images.append(big_img)\n",
        "\n",
        "for idx, vertical_image in enumerate(vertical_images):\n",
        "  print(\"idx\", idx)\n",
        "  if idx == 0:\n",
        "    big_img = vertical_image\n",
        "    continue\n",
        "  # if idx == 5:\n",
        "  #   break\n",
        "  print(\"big_img\", big_img)\n",
        "  print(\"vertical_image\", vertical_image)\n",
        "  big_img = get_concat_v(big_img, vertical_image)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvsdiv9Scgyh"
      },
      "outputs": [],
      "source": [
        "big_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZhRQlb_cz6-"
      },
      "source": [
        "## Train on the bigger image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0s6kny9dpiJ"
      },
      "outputs": [],
      "source": [
        "def image_tensor(img, sidelength):\n",
        "    print(img.size)\n",
        "    transform = Compose([\n",
        "        Resize(sidelength),\n",
        "        ToTensor(),\n",
        "        Normalize(torch.Tensor([0.5]), torch.Tensor([0.5]))\n",
        "    ])\n",
        "    \n",
        "    img = transform(img)\n",
        "    return img\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w77fJMlvdXub"
      },
      "outputs": [],
      "source": [
        "class ImageFitting(Dataset):\n",
        "    def __init__(self, img, img_size):\n",
        "        super().__init__()\n",
        "        img = image_tensor(img,img_size)\n",
        "        print(img.shape)\n",
        "        self.pixels = img.permute(1, 2, 0).view(-1, 3)\n",
        "        self.coords = get_mgrid(img_size, 2)\n",
        "\n",
        "    def __len__(self):\n",
        "        return 1\n",
        "\n",
        "    def __getitem__(self, idx):    \n",
        "        if idx > 0: raise IndexError\n",
        "            \n",
        "        return self.coords, self.pixels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xJgfF7Fc2ZX"
      },
      "outputs": [],
      "source": [
        "all_images = ImageFitting(big_img, 480)\n",
        "dataloader = DataLoader(all_images, batch_size=1, pin_memory=True, num_workers=0)\n",
        "\n",
        "# small architecture to handle bigger picture\n",
        "img_siren = Siren(in_features=2, out_features=3, hidden_features=64, \n",
        "                  hidden_layers=3, outermost_linear=True)\n",
        "#TBD: GPU\n",
        "img_siren.cuda() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pov6w9h0eI7C"
      },
      "outputs": [],
      "source": [
        "total_steps = 1000 # Since the whole image is our dataset, this just means 808 gradient descent steps.\n",
        "steps_til_summary = 10\n",
        "\n",
        "optim = torch.optim.Adam(lr=1e-4, params=img_siren.parameters())\n",
        "\n",
        "model_input, ground_truth = next(iter(dataloader))\n",
        "#TBD: GPU\n",
        "model_input, ground_truth = model_input.cuda(), ground_truth.cuda()\n",
        "#model_input, ground_truth = model_input, ground_truth\n",
        "\n",
        "for step in range(total_steps):\n",
        "    model_output, coords = img_siren(model_input)    \n",
        "    # print(\"model_output.shape: \" , model_output.shape)\n",
        "\n",
        "    # print(\"ground_truch: \", ground_truth)\n",
        "    # print(\"ground_truch.shape: \", ground_truth.shape)\n",
        "\n",
        "    loss = ((model_output - ground_truth)**2).mean()\n",
        "    \n",
        "    if not step % steps_til_summary:\n",
        "        print(\"Step %d, Total loss %0.6f\" % (step, loss))\n",
        "        # img_grad = gradient(model_output, coords)\n",
        "        # img_laplacian = laplace(model_output, coords)\n",
        "\n",
        "        fig, axes = plt.subplots(1,1, figsize=(18,6))\n",
        "        axes.imshow(model_output.cpu().view(480,480,3).detach().numpy())\n",
        "        # axes[1].imshow(img_grad.norm(dim=-1).cpu().view(480,480).detach().numpy())\n",
        "        # axes[2].imshow(img_laplacian.cpu().view(480,480).detach().numpy())\n",
        "        plt.show()\n",
        "\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"probing_solution\"></a>\n",
        "# Probing the \"Naive\" implicit function\n"
      ],
      "metadata": {
        "id": "i6NYNSWCCcLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upscaling"
      ],
      "metadata": {
        "id": "neAKfdYN7BFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_specific_coords(input_coords, horizontal , vertical, original_size, img_num = 0):\n",
        "  specific_coords = []\n",
        "\n",
        "  vertical_offset = int(( (img_num//10)) * (original_size*vertical) )\n",
        "  print(\"vertical_offset\", vertical_offset)\n",
        "\n",
        "  img_num_offset = img_num * horizontal + vertical_offset\n",
        "  for vertical_coords in range(vertical):    \n",
        "    specific_coords.append( input_coords[img_num_offset + vertical_coords*original_size : img_num_offset + vertical_coords*original_size + horizontal])    \n",
        "\n",
        "  return torch.stack(specific_coords, 0)"
      ],
      "metadata": {
        "id": "a2IGnWUKHq1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_OF_IMAGES_ON_AX = 10\n",
        "def get_img_cords(img_num, new_img_size,new_size_coords):\n",
        "  return get_specific_coords(new_size_coords,new_img_size,new_img_size,new_img_size*NUM_OF_IMAGES_ON_AX,img_num)\n"
      ],
      "metadata": {
        "id": "HDCoBT5IuZbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def up_sample_img(img_num,new_size):\n",
        "  new_size_coords = get_mgrid(new_size*10, 2)\n",
        "  #TBD: GPU\n",
        "  img_coords = get_img_cords(img_num,new_size,new_size_coords).cuda()\n",
        "  super_size_img1, _ = img_siren(img_coords)\n",
        "  return super_size_img1"
      ],
      "metadata": {
        "id": "Hxvw0f-75nCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "super_size_img1 = up_sample_img(85,255)\n",
        "fig, axes = plt.subplots(1,1, figsize=(18,6))\n",
        "axes.imshow(super_size_img1.cpu().view(255,255,3).detach().numpy())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gN_ENvTb6QW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "super_size_img1 = up_sample_img(32,255)\n",
        "fig, axes = plt.subplots(1,1, figsize=(18,6))\n",
        "axes.imshow(super_size_img1.cpu().view(255,255,3).detach().numpy())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aDHvbyQl7Slo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "super_size_img1 = up_sample_img(0,255)\n",
        "fig, axes = plt.subplots(1,1, figsize=(18,6))\n",
        "axes.imshow(super_size_img1.cpu().view(255,255,3).detach().numpy())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cXih0L4Y7WhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "results looks ugly, but remember this is only a baseline"
      ],
      "metadata": {
        "id": "nNGv4DSx5aJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now that we have this simple baseline model we can try a more serious solution"
      ],
      "metadata": {
        "id": "NyIybhkZ9CXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"improving_solution\"></a>\n",
        "# Improving the solution - A fully connected Sinus Activation Module with entity embedding for different images"
      ],
      "metadata": {
        "id": "X-YTAw-W1bzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mgrid(sidelen, dim=2):\n",
        "    '''Generates a flattened grid of (x,y,...) coordinates in a range of -1 to 1.\n",
        "    sidelen: int\n",
        "    dim: int'''\n",
        "    tensors = tuple(dim * [torch.linspace(-1, 1, steps=sidelen)])\n",
        "    mgrid = torch.stack(torch.meshgrid(*tensors), dim=-1)\n",
        "    mgrid = mgrid.reshape(-1, dim)\n",
        "    return mgrid"
      ],
      "metadata": {
        "id": "38MEYAf88qZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SineLayer(nn.Module):\n",
        "    # See paper sec. 3.2, final paragraph, and supplement Sec. 1.5 for discussion of omega_0.\n",
        "    \n",
        "    # If is_first=True, omega_0 is a frequency factor which simply multiplies the activations before the \n",
        "    # nonlinearity. Different signals may require different omega_0 in the first layer - this is a \n",
        "    # hyperparameter.\n",
        "    \n",
        "    # If is_first=False, then the weights will be divided by omega_0 so as to keep the magnitude of \n",
        "    # activations constant, but boost gradients to the weight matrix (see supplement Sec. 1.5)\n",
        "    \n",
        "    def __init__(self, in_features, out_features, bias=True,\n",
        "                 is_first=False, omega_0=30):\n",
        "        super().__init__()\n",
        "        self.omega_0 = omega_0\n",
        "        self.is_first = is_first\n",
        "        \n",
        "        self.in_features = in_features\n",
        "\n",
        "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
        "        \n",
        "\n",
        "        self.init_weights()\n",
        "    \n",
        "    def init_weights(self):\n",
        "        with torch.no_grad():\n",
        "            if self.is_first:\n",
        "                self.linear.weight.uniform_(-1 / self.in_features, \n",
        "                                             1 / self.in_features)      \n",
        "            else:\n",
        "                self.linear.weight.uniform_(-np.sqrt(6 / self.in_features) / self.omega_0, \n",
        "                                             np.sqrt(6 / self.in_features) / self.omega_0)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        return torch.sin(self.omega_0 * self.linear(input))\n",
        "    \n",
        "    def forward_with_intermediate(self, input): \n",
        "        # For visualization of activation distributions\n",
        "        intermediate = self.omega_0 * self.linear(input)\n",
        "        return torch.sin(intermediate), intermediate\n",
        "    \n",
        "    \n",
        "class Img_Representation(nn.Module):\n",
        "    def __init__(self, coords_features, img_num_embedding_size, hidden_features, hidden_layers, out_features, outermost_linear=False, \n",
        "                 first_omega_0=30, hidden_omega_0=30.):\n",
        "        super().__init__()\n",
        "        \n",
        "        #embedding layer for the image number\n",
        "        self.image_embedding = nn.Embedding(100, img_num_embedding_size)\n",
        "\n",
        "        self.net = []\n",
        "        self.net.append(SineLayer(coords_features + img_num_embedding_size, hidden_features, \n",
        "                                  is_first=True, omega_0=first_omega_0))\n",
        "\n",
        "        for i in range(hidden_layers):\n",
        "            self.net.append(SineLayer(hidden_features, hidden_features, \n",
        "                                      is_first=False, omega_0=hidden_omega_0))\n",
        "\n",
        "        if outermost_linear:\n",
        "            final_linear = nn.Linear(hidden_features, out_features)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                final_linear.weight.uniform_(-np.sqrt(6 / hidden_features) / hidden_omega_0, \n",
        "                                              np.sqrt(6 / hidden_features) / hidden_omega_0)\n",
        "                \n",
        "            self.net.append(final_linear)\n",
        "        else:\n",
        "            self.net.append(SineLayer(hidden_features, out_features, \n",
        "                                      is_first=False, omega_0=hidden_omega_0))\n",
        "        \n",
        "        self.net = nn.Sequential(*self.net)\n",
        "    \n",
        "    def forward(self,  coords_and_imagenum):\n",
        "#        print(\"len(coords_and_imagenum): \" , len(coords_and_imagenum) )\n",
        "\n",
        "#        print(\"coords_and_imagenum[0].shape: \" , coords_and_imagenum[0].shape )\n",
        "        #print(\"coords_and_imagenum[1].shape: \" , coords_and_imagenum[1].shape )\n",
        "\n",
        "#        print(\"before cutting: \", coords_and_imagenum.shape )\n",
        "        coords = coords_and_imagenum[:,:,1:3]\n",
        "\n",
        "#        print(\"coords after cutting, shape: \" , coords.shape)\n",
        "\n",
        "        img_num = coords_and_imagenum[:,:,0]\n",
        "        print(\"coords_and_imagenum after cutting, shape: \" , img_num.shape)\n",
        "\n",
        "        coords = coords.clone().detach().requires_grad_(True) # allows to take derivative w.r.t. input\n",
        "        output = self.net(coords)\n",
        "        return output, coords        \n",
        "#        return None, None\n",
        "\n",
        "    def forward_with_activations(self, coords_and_imagenum, retain_grad=False):\n",
        "        '''Returns not only model output, but also intermediate activations.\n",
        "        Only used for visualizing activations later!'''\n",
        "        activations = OrderedDict()\n",
        "\n",
        "        print(\"coords_and_imagenum.shape: \", coords_and_imagenum.shape)\n",
        "        coords = coords_and_imagenum[:,1:3]\n",
        "        img_num = coords_and_imagenum[:,0]\n",
        "\n",
        "        activation_count = 0\n",
        "        x = coords.clone().detach().requires_grad_(True)\n",
        "        \n",
        "        img_embedded = self.image_embedding(img_num)\n",
        "        activations['input'] = torch.cat((img_embedded,x), 0)\n",
        "\n",
        "        for i, layer in enumerate(self.net):\n",
        "            if isinstance(layer, SineLayer):\n",
        "                x, intermed = layer.forward_with_intermediate(x)\n",
        "                \n",
        "                if retain_grad:\n",
        "                    x.retain_grad()\n",
        "                    intermed.retain_grad()\n",
        "                    \n",
        "                activations['_'.join((str(layer.__class__), \"%d\" % activation_count))] = intermed\n",
        "                activation_count += 1\n",
        "            else: \n",
        "                x = layer(x)\n",
        "                \n",
        "                if retain_grad:\n",
        "                    x.retain_grad()\n",
        "                    \n",
        "            activations['_'.join((str(layer.__class__), \"%d\" % activation_count))] = x\n",
        "            activation_count += 1\n",
        "\n",
        "        return activations"
      ],
      "metadata": {
        "id": "cy4bTn3w9Qxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Img_Representation(nn.Module):\n",
        "    def __init__(self, coords_features, img_num_embedding_size, hidden_features, hidden_layers, out_features, outermost_linear=False, \n",
        "                 first_omega_0=30, hidden_omega_0=30.):\n",
        "        super().__init__()\n",
        "        \n",
        "        #embedding layer for the image number\n",
        "        self.image_embedding = nn.Embedding(100, img_num_embedding_size)\n",
        "\n",
        "        self.net = []\n",
        "        self.net.append(SineLayer(coords_features + img_num_embedding_size, hidden_features, \n",
        "                                  is_first=True, omega_0=first_omega_0))\n",
        "\n",
        "        for i in range(hidden_layers):\n",
        "            self.net.append(SineLayer(hidden_features, hidden_features, \n",
        "                                      is_first=False, omega_0=hidden_omega_0))\n",
        "\n",
        "        if outermost_linear:\n",
        "            final_linear = nn.Linear(hidden_features, out_features)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                final_linear.weight.uniform_(-np.sqrt(6 / hidden_features) / hidden_omega_0, \n",
        "                                              np.sqrt(6 / hidden_features) / hidden_omega_0)\n",
        "                \n",
        "            self.net.append(final_linear)\n",
        "        else:\n",
        "            self.net.append(SineLayer(hidden_features, out_features, \n",
        "                                      is_first=False, omega_0=hidden_omega_0))\n",
        "        \n",
        "        self.net = nn.Sequential(*self.net)\n",
        "    \n",
        "    def forward(self, coords_and_imagenum):\n",
        "\n",
        "        coords = coords_and_imagenum[:,:,1:3]\n",
        "        img_num = coords_and_imagenum[:,:,0].long()\n",
        "\n",
        "        img_embedded = self.image_embedding(img_num)\n",
        "\n",
        "        coords_and_img_num = torch.cat((img_embedded,coords), 2)\n",
        "        \n",
        "        coords = coords.clone().detach().requires_grad_(True) # allows to take derivative w.r.t. input\n",
        "        output = self.net(coords_and_img_num)\n",
        "        return output, coords        \n",
        "\n",
        "    def forward_with_activations(self, coords, retain_grad=False):\n",
        "        '''Returns not only model output, but also intermediate activations.\n",
        "        Only used for visualizing activations later!'''\n",
        "        activations = OrderedDict()\n",
        "\n",
        "        activation_count = 0\n",
        "        x = coords.clone().detach().requires_grad_(True)\n",
        "        activations['input'] = x\n",
        "        for i, layer in enumerate(self.net):\n",
        "            if isinstance(layer, SineLayer):\n",
        "                x, intermed = layer.forward_with_intermediate(x)\n",
        "                \n",
        "                if retain_grad:\n",
        "                    x.retain_grad()\n",
        "                    intermed.retain_grad()\n",
        "                    \n",
        "                activations['_'.join((str(layer.__class__), \"%d\" % activation_count))] = intermed\n",
        "                activation_count += 1\n",
        "            else: \n",
        "                x = layer(x)\n",
        "                \n",
        "                if retain_grad:\n",
        "                    x.retain_grad()\n",
        "                    \n",
        "            activations['_'.join((str(layer.__class__), \"%d\" % activation_count))] = x\n",
        "            activation_count += 1\n",
        "\n",
        "        return activations"
      ],
      "metadata": {
        "id": "lfn784CUHm7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_tensor(img, sidelength):\n",
        "    transform = Compose([\n",
        "        Resize(sidelength),\n",
        "        ToTensor(),\n",
        "        Normalize(torch.Tensor([0.5]), torch.Tensor([0.5]))\n",
        "    ])\n",
        "    \n",
        "    img = transform(img)\n",
        "    return img\n"
      ],
      "metadata": {
        "id": "wWHL1h3yRlBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERz6cN0x7q-8"
      },
      "outputs": [],
      "source": [
        "class ImageFitting(Dataset):\n",
        "    def __init__(self, images_path, img_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.pixels = torch.tensor([])\n",
        "        self.coords_and_img_num = torch.tensor([])\n",
        "        for img_num,img_file in enumerate(glob.glob(path)):\n",
        "          new_image = Image.fromarray(io.imread(img_file)).convert('RGB')\n",
        "          img = image_tensor(new_image,img_size)\n",
        "\n",
        "          img_pixels = img.permute(1, 2, 0).view(-1, 3)\n",
        "          img_coords = get_mgrid(img_size, 2)\n",
        "          img_img_num = torch.full((img_coords.shape[0], 1), img_num)\n",
        "          coords_and_img_num = torch.hstack((img_img_num, img_coords))\n",
        "          self.pixels = torch.cat((self.pixels, img_pixels),0)\n",
        "          self.coords_and_img_num = torch.cat((self.coords_and_img_num, coords_and_img_num),0)\n",
        "        print(\"self.coords_and_img_num.shape: \", self.coords_and_img_num.shape)\n",
        "    def __len__(self):\n",
        "        return 1\n",
        "\n",
        "    def __getitem__(self, idx):    \n",
        "        if idx > 0: raise IndexError\n",
        "            \n",
        "        return self.coords_and_img_num, self.pixels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_image_gt(img_file, img_size, img_num):\n",
        "  new_image = Image.fromarray(skimage.io.imread(file)).convert('RGB')\n",
        "  return ImageFitting(new_image, img_size, img_num) \n"
      ],
      "metadata": {
        "id": "ICBlM5HK8ljf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def up_sample(img_num,size,return_pixels = False):\n",
        "  new_size_coords = get_mgrid(size, 2)\n",
        "  img_img_num = torch.full((new_size_coords.shape[0], 1), img_num)\n",
        "  coords_and_img_num = torch.hstack((img_img_num, new_size_coords))\n",
        "  model_output, coords = img_representation(coords_and_img_num.unsqueeze(0).cuda())    \n",
        "\n",
        "  model_output_pixels = model_output.cpu()[:,:,:].view(size,size,3).detach().numpy()\n",
        "\n",
        "  if return_pixels:\n",
        "    return model_output_pixels, model_output.cpu()[:,:,:]\n",
        "  fig, axes = plt.subplots(1,1, figsize=(18,6))\n",
        "  axes.imshow(model_output_pixels)\n",
        "  #axes.imshow(super_size_img1.cpu().view(255,255,3).detach().numpy())\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "X8Vn7SuMdGTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Searching for architecture"
      ],
      "metadata": {
        "id": "KOBYi8kpPvZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# didn't generalize well enough\n",
        "# img_representation = Img_Representation(coords_features=2, img_num_embedding_size = 3, out_features=3, hidden_features=64, \n",
        "#                   hidden_layers=3, outermost_linear=True)\n",
        "# best so far\n",
        "img_representation = Img_Representation(coords_features=2, img_num_embedding_size = 4, out_features=3, hidden_features=100, \n",
        "                   hidden_layers=4, outermost_linear=True)\n",
        "# no serious improvement\n",
        "# img_representation = Img_Representation(coords_features=2, img_num_embedding_size = 4, out_features=3, hidden_features=200, \n",
        "#                    hidden_layers=6, outermost_linear=True)\n",
        "\n",
        "# img_representation = Img_Representation(coords_features=2, img_num_embedding_size = 4, out_features=3, hidden_features=300, \n",
        "#                    hidden_layers=8, outermost_linear=True)\n",
        "\n",
        "\n",
        "# img_representation = Img_Representation(coords_features=2, img_num_embedding_size = 7, out_features=3, hidden_features=120, \n",
        "#                    hidden_layers=4, outermost_linear=True)\n",
        "\n",
        "# seems to give ok results\n",
        "# img_representation = Img_Representation(coords_features=2, img_num_embedding_size = 4, out_features=3, hidden_features=100, \n",
        "#                    hidden_layers=8, outermost_linear=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "4XrcH8fsPqz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Arranging for training"
      ],
      "metadata": {
        "id": "Fd_9g6Db3Vtk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5t-dhnZ7q-9"
      },
      "outputs": [],
      "source": [
        "path = files_path + \"/48/*.*\"\n",
        "\n",
        "all_images = ImageFitting(path, 48)\n",
        "#print(\"all_images: \", all_images)\n",
        "dataloader = DataLoader(all_images, batch_size=1, pin_memory=True, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TBD: GPU\n",
        "img_representation.cuda() "
      ],
      "metadata": {
        "id": "PuDOWVyUEBl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop"
      ],
      "metadata": {
        "id": "DZPr2032FEqd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "reference super resolution image (255)"
      ],
      "metadata": {
        "id": "WUBMBOWzgp48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "high_res_image = Image.fromarray(io.imread(files_path + \"/256/workstation-256.png\")).convert('RGB')\n",
        "high_res_image_pixels = image_tensor(high_res_image,255).permute(1, 2, 0).view(-1, 3)\n",
        "fig, axes = plt.subplots(1,1, figsize=(18,6))\n",
        "axes.imshow(high_res_image)\n",
        "#axes.imshow(super_size_img1.cpu().view(255,255,3).detach().numpy())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L_kwuoBxgn5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Testing generalization by upscaling and comparison to high resolution with PSNR"
      ],
      "metadata": {
        "id": "UyhaZIz5OEm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_steps = 2500 \n",
        "steps_til_summary = 10\n",
        "psnr = PeakSignalNoiseRatio()\n",
        "\n",
        "\n",
        "optim = torch.optim.Adam(lr=1e-4, params=img_representation.parameters())\n",
        "\n",
        "model_input, ground_truth = next(iter(dataloader))\n",
        "#print(model_input.shape)\n",
        "#TBD: GPU\n",
        "model_input, ground_truth = model_input.cuda(), ground_truth.cuda()\n",
        "#model_input, ground_truth = model_input, ground_truth\n",
        "\n",
        "for step in range(total_steps):\n",
        "    model_output, coords = img_representation(model_input)    \n",
        "    # print(\"model_output.shape: \" , model_output.shape)\n",
        "\n",
        "    # print(\"ground_truch: \", ground_truth)\n",
        "    # print(\"ground_truch.shape: \", ground_truth.shape)\n",
        "\n",
        "    loss = ((model_output - ground_truth)**2).mean()\n",
        "    \n",
        "    if not step % steps_til_summary:\n",
        "        print(\"Step %d, Total loss %0.6f\" % (step, loss))\n",
        "        # img_grad = gradient(model_output, coords)\n",
        "        # img_laplacian = laplace(model_output, coords)\n",
        "\n",
        "        img_num = 99\n",
        "        start = img_num*48*48\n",
        "        end = (img_num+1)*48*48\n",
        "\n",
        "        fig, axes = plt.subplots(1,3, figsize=(18,6))\n",
        "\n",
        "        axes[0].imshow(model_output.cpu()[:,start:end,:].view(48,48,3).detach().numpy())\n",
        "        upsampled_pixels, upsampled_pixels_tensor = up_sample(img_num,255,return_pixels = True)\n",
        "\n",
        "        psnr_res = psnr(upsampled_pixels_tensor, high_res_image_pixels)\n",
        "        print(\"PSNR between upsample and high resolution: \", psnr_res)\n",
        "        axes[1].imshow(upsampled_pixels)\n",
        "        axes[2].imshow(high_res_image)\n",
        "        \n",
        "        \n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()"
      ],
      "metadata": {
        "id": "mn7bqvROFGfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load model instead of training..."
      ],
      "metadata": {
        "id": "vS7F2F9gOZTP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t-DKe--qxYk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model_path = files_path + \"/models/img_representation.pt\""
      ],
      "metadata": {
        "id": "JvFVp0TV33Zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#img_representation = torch.load(model_path)"
      ],
      "metadata": {
        "id": "V3lebTLdNmhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model_input, ground_truth = model_input.cuda(), ground_truth.cuda()\n"
      ],
      "metadata": {
        "id": "YvYovOiP6Sdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model_output, coords = img_representation(model_input)  "
      ],
      "metadata": {
        "id": "-4JufCAc6Tbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "number of parameters of the model"
      ],
      "metadata": {
        "id": "C0IkVU_P4iMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum(p.numel() for p in img_representation.parameters())"
      ],
      "metadata": {
        "id": "OJFSCvwuPbdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check output"
      ],
      "metadata": {
        "id": "ebaYJv6BPcWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_num =  37\n",
        "start = img_num*48*48\n",
        "end = (img_num+1)*48*48\n",
        "\n",
        "model_output_pixels = model_output.cpu()[:,start:end,:].view(48,48,3).detach().numpy()\n",
        "\n",
        "fig, axes = plt.subplots(1,1, figsize=(18,6))\n",
        "axes.imshow(model_output_pixels)\n",
        "#axes.imshow(super_size_img1.cpu().view(255,255,3).detach().numpy())\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bakiIhgEPbAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing upsampling "
      ],
      "metadata": {
        "id": "AWSRZEp5VG8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_img_pixels(img_num,size):\n",
        "  new_size_coords = get_mgrid(size, 2)\n",
        "  img_img_num = torch.full((new_size_coords.shape[0], 1), img_num)\n",
        "  coords_and_img_num = torch.hstack((img_img_num, new_size_coords))\n",
        "  model_output, coords = img_representation(coords_and_img_num.unsqueeze(0).cuda())    \n",
        "  model_output_pixels = model_output.cpu()[:,:,:].view(size,size,3).detach().numpy()\n",
        "\n",
        "  return model_output_pixels, model_output.cpu()[:,:,:]\n"
      ],
      "metadata": {
        "id": "UTt3uWbbc5rH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "up_sample(78,255)"
      ],
      "metadata": {
        "id": "X-_Mx41jyHYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "up_sample(83,255)"
      ],
      "metadata": {
        "id": "cP1Nt1X7UUpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"`interpolation`\"></a>\n",
        "# Interpolation and Similarity measures for representations\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "woiKqjDcLmd-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpolating"
      ],
      "metadata": {
        "id": "IDv8924qrQVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lerp(factor, a, b):\n",
        "  return factor*a + (1.0 - factor)*b"
      ],
      "metadata": {
        "id": "4YUVgMHnsX6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def interpolate_pixels(factor, size, src1, src2):\n",
        "  dest = torch.zeros(size, size,3)\n",
        "  for x in range(size):\n",
        "    for y in range(size):\n",
        "      dest[x,y,0] = lerp(factor, src1[x,y,0], src2[x,y,0])\n",
        "      dest[x,y,1] = lerp(factor, src1[x,y,1], src2[x,y,1])\n",
        "      dest[x,y,2] = lerp(factor, src1[x,y,2], src2[x,y,2])\n",
        "  return dest\n"
      ],
      "metadata": {
        "id": "SmeXF3n_skzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def interpolate_images(img1, img2):\n",
        "  # get img1\n",
        "  img_num =  img1\n",
        "\n",
        "  start = img_num*48*48\n",
        "  end = (img_num+1)*48*48\n",
        "\n",
        "  model_output_pixels_img_1 = model_output.cpu()[:,start:end,:].view(48,48,3).detach().numpy()\n",
        "\n",
        "  img_num =  img2\n",
        "\n",
        "  start = img_num*48*48\n",
        "  end = (img_num+1)*48*48\n",
        "\n",
        "  model_output_pixels_img_2 = model_output.cpu()[:,start:end,:].view(48,48,3).detach().numpy()\n",
        "\n",
        "  interpolated_image = interpolate_pixels(0.5, 48, model_output_pixels_img_1, model_output_pixels_img_2)\n",
        "\n",
        "  # show image\n",
        "  fig, axes = plt.subplots(1,1, figsize=(18,6))\n",
        "  axes.imshow(interpolated_image)\n",
        "  plt.show()  "
      ],
      "metadata": {
        "id": "_sxTK4VGuhwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interpolate_images(37, 85)"
      ],
      "metadata": {
        "id": "Jy7mU6g8wTJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interpolate_images(97, 95)"
      ],
      "metadata": {
        "id": "oA0h54_Nu17s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interpolate_images(22, 38)"
      ],
      "metadata": {
        "id": "LuxpmaXdF8-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Measuring distance between model activations of different images\n",
        "\n",
        "\n",
        "Didn't have time to get it to work :-( \n",
        "\n",
        "The idea was to use this approach to: # https://github.com/AntixK/PyTorch-Model-Compare to compare the representation of the layers over activations taken from different images\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4K5NX6xvPuW_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ewDjdjzQLErI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "id": "woAEPRTIYezM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir /content/gdrive/MyDrive/Github"
      ],
      "metadata": {
        "id": "a-b21S8pYtoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/Github"
      ],
      "metadata": {
        "id": "Co-cXaOBZfwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://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@github.com/AntixK/PyTorch-Model-Compare\n",
        "\n"
      ],
      "metadata": {
        "id": "MGJsoxEHZszl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd PyTorch-Model-Compare\n",
        " "
      ],
      "metadata": {
        "id": "FmAm9a51acA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OIxHcxHfQEJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd  /content/gdrive/MyDrive/Github/PyTorch-Model-Compare"
      ],
      "metadata": {
        "id": "nlGqNEWFQENm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageFitting(Dataset):\n",
        "    def __init__(self, images_path, img_size, only_img = None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.pixels = torch.tensor([])\n",
        "        self.coords_and_img_num = torch.tensor([])\n",
        "        for img_num,img_file in enumerate(glob.glob(path)):\n",
        "          new_image = Image.fromarray(io.imread(img_file)).convert('RGB')\n",
        "          #print(\"img_num:\", img_num, \"img_file: \", img_file )\n",
        "          img = image_tensor(new_image,img_size)\n",
        "\n",
        "          img_pixels = img.permute(1, 2, 0).view(-1, 3)\n",
        "          img_coords = get_mgrid(img_size, 2)\n",
        "          img_img_num = torch.full((img_coords.shape[0], 1), img_num)\n",
        "          coords_and_img_num = torch.hstack((img_img_num, img_coords))\n",
        "\n",
        "          \n",
        "          if only_img is None:\n",
        "            self.pixels = torch.cat((self.pixels, img_pixels),0)\n",
        "            self.coords_and_img_num = torch.cat((self.coords_and_img_num, coords_and_img_num),0)\n",
        "          #only be a loader for the only_img\n",
        "          elif only_img == img_num:\n",
        "            self.pixels = torch.cat((self.pixels, img_pixels),0)\n",
        "            self.coords_and_img_num = torch.cat((self.coords_and_img_num, coords_and_img_num),0)\n",
        "            #only for test\n",
        "            return\n",
        "\n",
        "    def __len__(self):\n",
        "        return 1\n",
        "\n",
        "    def __getitem__(self, idx):    \n",
        "        if idx > 0: raise IndexError\n",
        "            \n",
        "        return self.coords_and_img_num, self.pixels"
      ],
      "metadata": {
        "id": "NIQZX70r0e_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# img_representation = Img_Representation(coords_features=2, img_num_embedding_size = 3, out_features=3, hidden_features=64, \n",
        "#                   hidden_layers=3, outermost_linear=True)\n",
        "# img_representation1 = Img_Representation(coords_features=2, img_num_embedding_size = 4, out_features=3, hidden_features=200, \n",
        "#                    hidden_layers=4, outermost_linear=True)\n",
        "\n",
        "# img_representation2 = Img_Representation(coords_features=2, img_num_embedding_size = 4, out_features=3, hidden_features=200, \n",
        "#                    hidden_layers=4, outermost_linear=True)"
      ],
      "metadata": {
        "id": "8ITwlAIu0Iqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = img_representation\n",
        "model2 = img_representation"
      ],
      "metadata": {
        "id": "oTg78KcZFD6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = files_path + \"/48/*.*\"\n",
        "\n",
        "image0 = ImageFitting(path, 48, only_img = 0)\n",
        "dataloader1 = DataLoader(image0, batch_size=1, pin_memory=True, num_workers=0)\n",
        "print(\"image0.pixels.shape\", image0.pixels.shape)\n",
        "\n",
        "image1 = ImageFitting(path, 48, only_img = 1)\n",
        "dataloader2 = DataLoader(image0, batch_size=1, pin_memory=True, num_workers=0)\n",
        "print(\"image1.pixels.shape\", image1.pixels.shape)\n"
      ],
      "metadata": {
        "id": "EaibmC_60Puq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_cka import CKA\n"
      ],
      "metadata": {
        "id": "wmEhXsrk4DHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model1)"
      ],
      "metadata": {
        "id": "mPmOGxG15JsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cka = CKA(model1, model2,\n",
        "          model1_name=\"img_representation1\",   # good idea to provide names to avoid confusion\n",
        "          model2_name=\"img_representation2\",   \n",
        "          model1_layers=[\"0\",\"1\",\"2\"], # List of layers to extract features from\n",
        "          model2_layers=[\"0\",\"1\",\"2\"], # extracts all layer features by default\n",
        "          device='cuda')\n",
        "\n"
      ],
      "metadata": {
        "id": "3DC39y4Bvp-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cka.compare(dataloader1, dataloader2) # secondary dataloader is optional\n",
        "\n",
        "results = cka.export()  # returns a dict that contains model names, layer names\n",
        "                        # and the CKA matrix\n"
      ],
      "metadata": {
        "id": "VEjxm0CDvrlO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}